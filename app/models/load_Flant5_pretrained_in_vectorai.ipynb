{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# Read JSONL file\n",
    "data = []\n",
    "with open('sample_data/ont_1_university_train.jsonl', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "# Convert to DataFrame and save as CSV\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('sample_data/ont_1_university_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "FILE = \"ont_1_university_test\"\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy()\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value.encode()]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "# Read the CSV\n",
    "df = pd.read_csv(f'sample_data/{FILE}.csv')\n",
    "\n",
    "# Write to TFRecord\n",
    "with tf.io.TFRecordWriter(f'sample_data/{FILE}.tfrecord') as writer:\n",
    "    for _, row in df.iterrows():\n",
    "        # Safely convert string representation of list to actual Python list\n",
    "        # triples_list = ast.literal_eval(row['triples'])\n",
    "        # Convert to JSON string\n",
    "        # formatted_triples = json.dumps(triples_list)\n",
    "        \n",
    "        feature = {\n",
    "            'id': _bytes_feature(row['id']),\n",
    "            'sent': _bytes_feature(row['sent']),\n",
    "           # 'triples': _bytes_feature(formatted_triples)\n",
    "        }\n",
    "        \n",
    "        example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "        writer.write(example.SerializeToString())\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, verify your TFRecord format is correct\n",
    "import tensorflow as tf\n",
    "\n",
    "# Read and verify the TFRecord\n",
    "raw_dataset = tf.data.TFRecordDataset(f'sample_data/{FILE}.tfrecord')\n",
    "\n",
    "# The feature description Flan-T5 typically expects\n",
    "feature_description = {\n",
    "    'id': tf.io.FixedLenFeature([], tf.string),\n",
    "    'sent': tf.io.FixedLenFeature([], tf.string),\n",
    "    #'triples': tf.io.FixedLenFeature([], tf.string),  # If you have this field\n",
    "}\n",
    "\n",
    "def _parse_function(example_proto):\n",
    "    return tf.io.parse_single_example(example_proto, feature_description)\n",
    "\n",
    "# Parse and check the data\n",
    "parsed_dataset = raw_dataset.map(_parse_function)\n",
    "for parsed_record in parsed_dataset.take(2):\n",
    "    print(parsed_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import storage\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import T5Config, TFT5ForConditionalGeneration\n",
    "\n",
    "import os\n",
    "import vertexai\n",
    "\n",
    "# 1. Set up paths and configurations\n",
    "LOCAL_MODEL_DIR = os.path.abspath(\"local_model\")\n",
    "SAVED_MODEL_DIR = os.path.abspath(\"saved_model\")\n",
    "GCS_BUCKET = \"ontologykg2\"\n",
    "GCS_MODEL_PATH = f\"gs://{GCS_BUCKET}/models/flan-t5-xxl\"\n",
    "PROJECT_ID = \"deft-return-439619-h9\"\n",
    "REGION = \"us-west1\"\n",
    "\n",
    "# Set up paths using os.path.join for cross-platform compatibility\n",
    "LOCAL_CHECKPOINT_DIR = os.path.abspath(\"local_checkpoint/checkpoint_1000001\")  # Use absolute path\n",
    "LOCAL_EXPORT_DIR = os.path.abspath(\"exported_model\")  # Use absolute path\n",
    "\n",
    "print(f\"Local model dir: {LOCAL_MODEL_DIR}\")\n",
    "print(f\"SavedModel dir: {SAVED_MODEL_DIR}\")\n",
    "\n",
    "print(f\"local checkpoint dir:{LOCAL_CHECKPOINT_DIR}\")\n",
    "\n",
    "# 2. Download and save FLAN-T5-base in .h5 format\n",
    "print(\"Downloading FLAN-T5-base...\")\n",
    "model = TFT5ForConditionalGeneration.from_pretrained(\n",
    "    \"google/flan-t5-base\",\n",
    "    from_pt=True  # Convert from PyTorch to TF\n",
    ")\n",
    "\n",
    "# Save as .h5\n",
    "os.makedirs(LOCAL_MODEL_DIR, exist_ok=True)\n",
    "model.save_pretrained(LOCAL_MODEL_DIR)\n",
    "print(f\"Model saved to {LOCAL_MODEL_DIR}\")\n",
    "\n",
    "# 3. Convert to SavedModel format\n",
    "print(\"Converting to SavedModel format...\")\n",
    "os.makedirs(SAVED_MODEL_DIR, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.flant5.FlanT5 import T5FineTuner, tokenize_dataset\n",
    "\n",
    "CKPT_PATH = \"models/flant5/epoch=3-step=2072-train_loss=0.35.ckpt\"\n",
    "#CKPT_PATH = \"lightning_logs/version_34/epoch=3-step=2072-train_loss=0.35.ckpt\"\n",
    "#CKPT_PATH = \"lightning_logs/version_30/final.ckpt\"\n",
    "\n",
    "checkpoint = torch.load(CKPT_PATH)\n",
    "print(checkpoint.keys())\n",
    "\n",
    "llm = T5FineTuner.load_from_checkpoint(CKPT_PATH)\n",
    "\n",
    "llm.model.eval() # set model to evaluation mode\n",
    "llm = llm.to(\"cpu\") # use CPU since I don't have GPU\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = llm.model\n",
    "\n",
    "# 1. Convert PyTorch model to TensorFlow\n",
    "def convert_pt_to_tf(llm):\n",
    "    \"\"\"Convert PyTorch T5 model to TensorFlow\"\"\"\n",
    "    from transformers import TFT5ForConditionalGeneration\n",
    "    \n",
    "    # Create TF model with same config\n",
    "    tf_model = TFT5ForConditionalGeneration.from_pretrained(\n",
    "        llm.hparam.model_name_or_path,\n",
    "        from_pt=True,\n",
    "        config=llm.model.config\n",
    "    )\n",
    "    \n",
    "    # Verify the conversion\n",
    "    print(\"Model converted from PyTorch to TensorFlow\")\n",
    "    print(f\"Model type: {type(tf_model)}\")\n",
    "\n",
    "    return tf_model\n",
    "\n",
    "# Convert PyTorch model to TF\n",
    "tf_model = convert_pt_to_tf(llm)\n",
    "model = tf_model  # Use converted model for serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(input_signature=[{\n",
    "    'input_ids': tf.TensorSpec(shape=(1, 512), dtype=tf.int32, name='input_ids'),\n",
    "    'attention_mask': tf.TensorSpec(shape=(1, 512), dtype=tf.int32, name='attention_mask')\n",
    "}])\n",
    "def serving_fn(inputs):\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_length=128,\n",
    "        num_beams=4,\n",
    "        pad_token_id=model.config.pad_token_id,\n",
    "        eos_token_id=model.config.eos_token_id,\n",
    "        bos_token_id=model.config.bos_token_id,\n",
    "        use_cache=True,\n",
    "        do_sample=False,\n",
    "        num_return_sequences=1,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True\n",
    "    )\n",
    "    return {'sequences': outputs.sequences}\n",
    "\n",
    "\n",
    "# 3. Test the serving function\n",
    "def test_serving_fn():\n",
    "    # Create dummy input\n",
    "    dummy_input = {\n",
    "        'input_ids': tf.zeros((1, 512), dtype=tf.int32),\n",
    "        'attention_mask': tf.ones((1, 512), dtype=tf.int32)\n",
    "    }\n",
    "    \n",
    "    # Test the function\n",
    "    result = serving_fn(dummy_input)\n",
    "    print(\"Serving function test successful\")\n",
    "    return result\n",
    "\n",
    "# Test before saving\n",
    "test_serving_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Save as SavedModel\n",
    "tf.saved_model.save(\n",
    "    model,\n",
    "    SAVED_MODEL_DIR,\n",
    "    signatures={\n",
    "        'serving_default': serving_fn\n",
    "    }\n",
    ")\n",
    "print(f\"SavedModel saved to {SAVED_MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create serving function\n",
    "\"\"\"\n",
    "@tf.function(input_signature=[{\n",
    "    'input_ids': tf.TensorSpec(shape=(None, None), dtype=tf.int32, name='input_ids'),\n",
    "    'attention_mask': tf.TensorSpec(shape=(None, None), dtype=tf.int32, name='attention_mask')\n",
    "}])\n",
    "def serving_fn(inputs):\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_length=128,\n",
    "        num_beams=4,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True\n",
    "    )\n",
    "    return {'sequences': outputs.sequences}\n",
    "\"\"\"\n",
    "GCS_BUCKET = \"ontologykg2\"\n",
    "GCS_MODEL_PATH = f\"gs://{GCS_BUCKET}/model/flant5\"\n",
    "\n",
    "# 4. Upload to GCS and deploy to Vertex AI\n",
    "print(\"Uploading to GCS and deploying to Vertex AI...\")\n",
    "vertexai.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "# Upload and create model\n",
    "model = aiplatform.Model.upload(\n",
    "    display_name=\"flan-t5-base\",\n",
    "    artifact_uri=GCS_MODEL_PATH,\n",
    "    serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/tf2-gpu.2-12:latest\", \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Deploy to endpoint\n",
    "endpoint = model.deploy(\n",
    "    machine_type=\"n1-standard-4\",  # Using GPU machine type for base\n",
    "    accelerator_type=\"NVIDIA_TESLA_T4\",\n",
    "    accelerator_count=1,\n",
    "    min_replica_count=1,\n",
    "    max_replica_count=1,\n",
    ")\n",
    "\n",
    "print(f\"Model deployed to endpoint: {endpoint.resource_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flant5 = FlanT5Client() #uses FLANT5_ENDPOINT set in utils.rag_constants.py by default\n",
    "# Test prediction\n",
    "test_text = \"Translate to French: Hello, how are you?\"\n",
    "response = flant5.generate_content(test_text)\n",
    "print(f\"Test prediction response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCS_EXPORT_DIR = \"\"\n",
    "# Now upload the model to Vertex AI\n",
    "model = aiplatform.Model.upload(\n",
    "    display_name=\"flan-t5-custom\",\n",
    "    artifact_uri=GCS_EXPORT_DIR,\n",
    "    serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-12:latest\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.language_models import TextGenerationModel  # Changed import\n",
    "from vertexai.generative_models import GenerativeModel\n",
    "\n",
    "from app.utils.rag_constants import PROJECT_ID, REGION, GENERATIVE_MODEL\n",
    "\n",
    "user_query = \"What's a good name for a flower shop that specializes in selling bouquets of dried flowers?\"\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "MODEL_ID = \"flant5-finetuned-model\"\n",
    "# Method 2: Using a model that you've fine-tuned\n",
    "model = vertexai.Model(\n",
    "    #name=f\"projects/{PROJECT_ID}/locations/{REGION}/models/{MODEL_ID}\",\n",
    "    name = endpoint\n",
    ")\n",
    "response = model.predict(instances=user_query)\n",
    "\n",
    "\"\"\"\n",
    "# Use pre-trained Flan-T5 directly\n",
    "model = TextGenerationModel.from_pretrained(\"text-bison@001\")  # or \"text-bison@latest\"\n",
    "response = model.predict(user_query)\n",
    "\n",
    "GENERATIVE_MODEL = \"gemini-1.5-flash-002\"\n",
    "model = GenerativeModel(GENERATIVE_MODEL)\n",
    "\n",
    "response = model.generate_content(\n",
    "    \"What's a good name for a flower shop that specializes in selling bouquets of dried flowers?\"\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "print(response.text)\n",
    "# Example response:\n",
    "# **Emphasizing the Dried Aspect:**\n",
    "# * Everlasting Blooms\n",
    "# * Dried & Delightful\n",
    "# * The Petal Preserve\n",
    "# ..."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
