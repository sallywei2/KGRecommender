{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dataset_parser import DatasetParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../merged_datasets2.csv\"\n",
    "data_cleaner = DatasetParser()\n",
    "\n",
    "# to pick up from where a previous run. By default loads from dataset_parser.PICKLE_FILE_PATH\n",
    "#data_cleaner.load_from_file('saved_dictionary_2.pkl')\n",
    "\n",
    "# default output is ta test_cleaned_table_output.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunks starting at chunk 0\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 10000/10000 rows into table\n",
      "loaded 6775/6775 rows into table\n",
      "Chunks processed: 272\n",
      "Saved parsed dataset to file: saved_dictionary_3.pkl\n"
     ]
    }
   ],
   "source": [
    "# process the datasset\n",
    "# -- parse individual features in the 'features' column\n",
    "# -- combine duplicate features and standardize names like finish type -> finish types\n",
    "\n",
    "#ITERATION_START = iter # set to a different number if picking up from a previous run\n",
    "ITERATION_START = 0\n",
    "\n",
    "MAX_ITERATIONS = 1\n",
    "iter = 0\n",
    "print(f\"Processing chunks starting at chunk {ITERATION_START}\")\n",
    "try:\n",
    "    for chunk in pd.read_csv(file_path, chunksize=10000):\n",
    "        if iter < ITERATION_START:\n",
    "            iter = iter + 1\n",
    "            continue\n",
    "        chunk.head()\n",
    "        data_cleaner.table = [] # reset table\n",
    "        data_cleaner.format_df_chunk(chunk)\n",
    "        data_cleaner.write_table_to_file() # selected_cols = selected_cols\n",
    "        iter = iter + 1\n",
    "        #if iter == MAX_ITERATIONS:\n",
    "        #    break\n",
    "except Exception as e:\n",
    "    raise e\n",
    "finally:\n",
    "    print(f\"Chunks processed: {iter}\")\n",
    "\n",
    "# save current progress to file. 46m 16.9s for first run.\n",
    "#data_cleaner.save_to_file('saved_dictionary.pkl') # initial run\n",
    "data_cleaner.save_to_file('saved_dictionary_3.pkl') # cleaned up features with only 1 count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cols = [1, 2, 3, 4\n",
    "                    , 5     # Product Dimensions {'5.63 x 2.83 x 0.39 inches; 1.9 Ounces'}\n",
    "                    , 120, 300, 350\n",
    "                    , 137 # Title: \"Skully's Ctz Beard Oil\"\n",
    "                    , 9000\n",
    "                    , 10000-1\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>items</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>572225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abpa_partslink_number</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ac_adapter_current</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>access_location</td>\n",
       "      <td>2459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>accessory</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   items  counts\n",
       "0                         572225\n",
       "1  abpa_partslink_number       3\n",
       "2     ac_adapter_current      33\n",
       "3        access_location    2459\n",
       "4              accessory       5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the set to console\n",
    "# data_cleaner.print_seen_attributes() \n",
    "\n",
    "# save set of attributes to file\n",
    "sorted_dict = dict(sorted(data_cleaner.set_of_seen_attributes.set.items(), key=lambda item: item))\n",
    "items = []\n",
    "counts = []\n",
    "for item in sorted_dict:\n",
    "    items.append(item)\n",
    "    counts.append(sorted_dict[item])\n",
    "\n",
    "\n",
    "df_att = pd.DataFrame.from_dict({ \"items\": items, \"counts\": counts})\n",
    "df_att.to_csv(\"saved_dictionary_feature_counts_3.csv\")\n",
    "df_att.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "    title: {'Replacement Left Right Trigger Shoulder Button L R Flex Cable for Sony PSV PS VITA 1000 PSVita 1000'}\n",
      "    main_category: {'Industrial & Scientific'}\n",
      "    average_rating: {4.8}\n",
      "    best_sellers_rank: {('item_model_number', '1'), ('rating_number', 11)}\n",
      "    price: {'11.98'}\n",
      "    store: {'SZLG'}\n",
      "    parent_asin: {'B07TS9YK6V'}\n",
      "    possible_category: {'author', 'bought_together', 'subtitle'}\n",
      "    brand: {'SZLG'}\n",
      "    color: {'Black'}\n",
      "    connector_gender: {'Male-to-Female'}\n",
      "    pieces: {'1.0 Count'}\n",
      "    indoor/outdoor_usage: {'Outdoor, Indoor'}\n",
      "    is_discontinued_by_manufacturer: {'No'}\n",
      "    package_dimensions: {'2.75 x 1.3 x 0.2 inches', '0.63 Ounces'}\n",
      "    date_first_available: {'November 10, 2019'}\n",
      "    manufacturer: {'SZLG'}\n",
      "    categories: {('Video Games', 'Legacy Systems', 'PlayStation Systems', 'PlayStation Vita', 'Games')}\n",
      "    description: {''}\n"
     ]
    }
   ],
   "source": [
    "#set_items = data_cleaner.set_of_seen_attributes.set\n",
    "#sorted_dict = dict(sorted(set_items.items(), key=lambda item: item))\n",
    "#for item in sorted_dict:\n",
    "#    print(item, sorted_dict[item])\n",
    "\n",
    "#data_cleaner.print_seen_attributes() # set\n",
    "\n",
    "print(i)\n",
    "data_cleaner.table[i].print()\n",
    "i = i + 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
