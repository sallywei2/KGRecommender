{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tensorflow as tf\n",
    "from google.cloud import aiplatform\n",
    "import vertexai\n",
    "from FlanT5 import T5FineTuner, tokenize_dataset\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Set up paths and configurations\n",
    "LOCAL_MODEL_DIR = os.path.abspath(\"local_model\")\n",
    "SAVED_MODEL_DIR = os.path.abspath(\"saved_model\")\n",
    "GCS_BUCKET = \"ontologykg2\"\n",
    "GCS_MODEL_PATH = f\"gs://{GCS_BUCKET}/model/flant5\"\n",
    "PROJECT_ID = \"deft-return-439619-h9\"\n",
    "REGION = \"us-west1\"\n",
    "\n",
    "# Set up paths using os.path.join for cross-platform compatibility\n",
    "LOCAL_CHECKPOINT_DIR = os.path.abspath(\"local_checkpoint/checkpoint_1000001\")  # Use absolute path\n",
    "LOCAL_EXPORT_DIR = os.path.abspath(\"exported_model\")  # Use absolute path\n",
    "\n",
    "CKPT_PATH = \"epoch=3-step=2072-train_loss=0.35.ckpt\"\n",
    "#CKPT_PATH = \"lightning_logs/version_34/epoch=3-step=2072-train_loss=0.35.ckpt\"\n",
    "#CKPT_PATH = \"lightning_logs/version_30/final.ckpt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load custom model from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'loops', 'callbacks', 'optimizer_states', 'lr_schedulers', 'hparams_name', 'hyper_parameters'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lawfu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# load the local model\n",
    "\n",
    "checkpoint = torch.load(CKPT_PATH)\n",
    "print(checkpoint.keys())\n",
    "\n",
    "llm = T5FineTuner.load_from_checkpoint(CKPT_PATH)\n",
    "\n",
    "llm.model.eval() # set model to evaluation mode\n",
    "llm = llm.to(\"cpu\") # use CPU since I don't have GPU\n",
    "print(\"Done\")\n",
    "\n",
    "model = llm.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pretrained model from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lawfu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\lawfu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tf_keras\\src\\initializers\\initializers.py:121: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n",
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model type: <class 'transformers.models.t5.modeling_tf_t5.TFT5ForConditionalGeneration'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lawfu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\generation\\tf_utils.py:837: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length.  recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> Wie old sind Sie?</s>\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import TFT5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "#model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\")\n",
    "tf_model = TFT5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "print(f\"Model type: {type(tf_model)}\")\n",
    "\n",
    "input_text = \"translate English to German: How old are you?\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "outputs = tf_model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Convert PyTorch model to TensorFlow\n",
    "def convert_pt_to_tf(llm):\n",
    "    \"\"\"Convert PyTorch T5 model to TensorFlow\"\"\"\n",
    "    from transformers import TFT5ForConditionalGeneration\n",
    "    \n",
    "    # Create TF model with same config\n",
    "    tf_model = TFT5ForConditionalGeneration.from_pretrained(\n",
    "        llm.hparam.model_name_or_path,\n",
    "        from_pt=True,\n",
    "        config=llm.model.config\n",
    "    )\n",
    "    \n",
    "    # Verify the conversion\n",
    "    print(\"Model converted from PyTorch to TensorFlow\")\n",
    "    print(f\"Model type: {type(tf_model)}\")\n",
    "\n",
    "    return tf_model\n",
    "\n",
    "# Convert PyTorch model to TF\n",
    "tf_model = convert_pt_to_tf(llm)\n",
    "model = tf_model  # Use converted model for serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lawfu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving function test successful\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequences': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[   0,   27,  183,  479,   21,    3,    9, 1876,  895, 3255,   38,\n",
       "            3,    9, 3591, 1876,   21,    3,    9, 4346,    5,    1,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0]])>}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tf.function(input_signature=[{\n",
    "    'input_ids': tf.TensorSpec(shape=(1,512), dtype=tf.int32, name='input_ids'),\n",
    "    'attention_mask': tf.TensorSpec(shape=(1,512),dtype=tf.int32, name='attention_mask')\n",
    "}])\n",
    "def serving_fn(inputs):\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_length=128,\n",
    "        num_beams=4,\n",
    "        pad_token_id=model.config.pad_token_id,\n",
    "        eos_token_id=model.config.eos_token_id,\n",
    "        bos_token_id=model.config.bos_token_id,\n",
    "        use_cache=True,\n",
    "        do_sample=False,\n",
    "        num_return_sequences=1,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True\n",
    "    )\n",
    "    return {'sequences': outputs.sequences}\n",
    "\n",
    "\n",
    "from CustomDataset import CustomDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/flan-t5-small')\n",
    "\n",
    "#from app.utils.flant5_client import FlanT5Client\n",
    "def _get_instance(text):\n",
    "    \"\"\"\n",
    "    Reformats text in FlanT5's CustomDataset format\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(data={'title':'', 'sent':text}, index=[0])\n",
    "    dataset = CustomDataset(tokenizer=tokenizer, dataset=df, type_path='test')\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=1, num_workers=1, shuffle=False)\n",
    "    for batch in dataloader:\n",
    "        input_ids =  batch['source_ids']\n",
    "        attention_mask =  batch['source_mask']\n",
    "        break\n",
    "    \n",
    "    return input_ids, attention_mask\n",
    "\n",
    "# 2.5 Test the serving function\n",
    "def test_serving_fn():\n",
    "\n",
    "    user_query = \"I am looking for a gift card suitable as a birthday gift for a writer.\"\n",
    "    input_ids, attention_mask = _get_instance(user_query)\n",
    "\n",
    "    # Create dummy input\n",
    "    dummy_input = {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask\n",
    "    }\n",
    "    \n",
    "    # Test the function\n",
    "    result = serving_fn(dummy_input)\n",
    "    print(\"Serving function test successful\")\n",
    "    return result\n",
    "\n",
    "# Test before saving\n",
    "test_serving_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c:\\Users\\lawfu\\Documents\\_Library\\_School\\SJSU MS Data Analytics\\202403_Fall_DATA-298B_FinalProject\\KGRecommender\\models\\flant5\\saved_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c:\\Users\\lawfu\\Documents\\_Library\\_School\\SJSU MS Data Analytics\\202403_Fall_DATA-298B_FinalProject\\KGRecommender\\models\\flant5\\saved_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SavedModel saved to c:\\Users\\lawfu\\Documents\\_Library\\_School\\SJSU MS Data Analytics\\202403_Fall_DATA-298B_FinalProject\\KGRecommender\\models\\flant5\\saved_model\n"
     ]
    }
   ],
   "source": [
    "# 3. Save as SavedModel format for VectorAI\n",
    "tf.saved_model.save(\n",
    "    model,\n",
    "    SAVED_MODEL_DIR\n",
    ")\n",
    "print(f\"SavedModel saved to {SAVED_MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually upload the SavedModel savedmodel.pb to the GSC Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading to GCS and deploying to Vertex AI...\n",
      "Creating Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Model backing LRO: projects/371748443295/locations/us-west1/models/2985046526063017984/operations/104966526813077504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Create Model backing LRO: projects/371748443295/locations/us-west1/models/2985046526063017984/operations/104966526813077504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created. Resource name: projects/371748443295/locations/us-west1/models/2985046526063017984@1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Model created. Resource name: projects/371748443295/locations/us-west1/models/2985046526063017984@1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use this Model in another session:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:To use this Model in another session:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model = aiplatform.Model('projects/371748443295/locations/us-west1/models/2985046526063017984@1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:model = aiplatform.Model('projects/371748443295/locations/us-west1/models/2985046526063017984@1')\n"
     ]
    }
   ],
   "source": [
    "GCS_BUCKET = \"ontologykg2\"\n",
    "GCS_MODEL_PATH = f\"gs://{GCS_BUCKET}/model/flant5\"\n",
    "\n",
    "# 4. Upload to GCS and deploy to Vertex AI\n",
    "print(\"Uploading to GCS and deploying to Vertex AI...\")\n",
    "vertexai.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "# Upload the model to Vertex AI's Model Registry\n",
    "model = aiplatform.Model.upload(\n",
    "    display_name=\"flan-t5-base\",\n",
    "    artifact_uri=GCS_MODEL_PATH,\n",
    "    serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/tf2-gpu.2-12:latest\", \n",
    ")\n",
    "\n",
    "# Specify your existing endpoint ID\n",
    "PROJECT_ID = \"371748443295\"\n",
    "ENDPOINT_REGION = \"us-west1\"\n",
    "ENDPOINT_ID = \"8518052919822516224\"\n",
    "# Retrieve the existing endpoint\n",
    "endpoint = aiplatform.Endpoint(endpoint_name=f\"projects/{PROJECT_ID}/locations/{ENDPOINT_REGION}/endpoints/{ENDPOINT_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually deploy the model to the endpoint in GSC Console to set min_replica_co9unt to none.\n",
    "\n",
    "GSC Consol > Vertex AI > Model Registry > flan-t5-base > Deploy & Test > Deploy to Endpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the model to the endpoint\n",
    "model.deploy(\n",
    "    endpoint=endpoint,\n",
    "    machine_type=\"n1-standard-4\",  # 4vCPUs, 15GB memory\n",
    "    accelerator_type=\"NVIDIA_TESLA_T4\",\n",
    "    accelerator_count=1,\n",
    "    min_replica_count=0, # fails.\n",
    "    #max_replica_count=1, # leave max replica count blank to turn off auto-scaling\n",
    ")\n",
    "\n",
    "print(f\"Model deployed to endpoint: {endpoint.resource_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    Uploading to GCS and deploying to Vertex AI...\n",
    "    Creating Model\n",
    "    INFO:google.cloud.aiplatform.models:Creating Model\n",
    "    Create Model backing LRO: projects/371748443295/locations/us-west1/models/5844832289443282944/operations/7813018026546036736\n",
    "    INFO:google.cloud.aiplatform.models:Create Model backing LRO: projects/371748443295/locations/us-west1/models/5844832289443282944/operations/7813018026546036736\n",
    "    Model created. Resource name: projects/371748443295/locations/us-west1/models/5844832289443282944@1\n",
    "    INFO:google.cloud.aiplatform.models:Model created. Resource name: projects/371748443295/locations/us-west1/models/5844832289443282944@1\n",
    "    To use this Model in another session:\n",
    "    INFO:google.cloud.aiplatform.models:To use this Model in another session:\n",
    "    model = aiplatform.Model('projects/371748443295/locations/us-west1/models/5844832289443282944@1')\n",
    "    INFO:google.cloud.aiplatform.models:model = aiplatform.Model('projects/371748443295/locations/us-west1/models/5844832289443282944@1')\n",
    "   \n",
    "    Creating Endpoint\n",
    "    INFO:google.cloud.aiplatform.models:Creating Endpoint\n",
    "    Create Endpoint backing LRO: projects/371748443295/locations/us-west1/endpoints/8518052919822516224/operations/8211586593568325632\n",
    "    INFO:google.cloud.aiplatform.models:Create Endpoint backing LRO: projects/371748443295/locations/us-west1/endpoints/8518052919822516224/operations/8211586593568325632\n",
    "    Endpoint created. Resource name: projects/371748443295/locations/us-west1/endpoints/8518052919822516224\n",
    "    INFO:google.cloud.aiplatform.models:Endpoint created. Resource name: projects/371748443295/locations/us-west1/endpoints/8518052919822516224\n",
    "    To use this Endpoint in another session:\n",
    "    INFO:google.cloud.aiplatform.models:To use this Endpoint in another session:\n",
    "    endpoint = aiplatform.Endpoint('projects/371748443295/locations/us-west1/endpoints/8518052919822516224')\n",
    "    INFO:google.cloud.aiplatform.models:endpoint = aiplatform.Endpoint('projects/371748443295/locations/us-west1/endpoints/8518052919822516224')\n",
    "    Deploying model to Endpoint : projects/371748443295/locations/us-west1/endpoints/8518052919822516224\n",
    "    INFO:google.cloud.aiplatform.models:Deploying model to Endpoint : projects/371748443295/locations/us-west1/endpoints/8518052919822516224\n",
    "    Deploy Endpoint model backing LRO: projects/371748443295/locations/us-west1/endpoints/8518052919822516224/operations/1435920954189414400\n",
    "    INFO:google.cloud.aiplatform.models:Deploy Endpoint model backing LRO: projects/371748443295/locations/us-west1/endpoints/8518052919822516224/operations/1435920954189414400\n",
    "    Endpoint model deployed. Resource name: projects/371748443295/locations/us-west1/endpoints/8518052919822516224\n",
    "    INFO:google.cloud.aiplatform.models:Endpoint model deployed. Resource name: projects/371748443295/locations/us-west1/endpoints/8518052919822516224\n",
    "    Model deployed to endpoint: projects/371748443295/locations/us-west1/endpoints/8518052919822516224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flant5 = FlanT5Client() #uses FLANT5_ENDPOINT set in utils.rag_constants.py by default\n",
    "# Test prediction\n",
    "test_text = \"Translate to French: Hello, how are you?\"\n",
    "response = model.generate_content(test_text)\n",
    "print(f\"Test prediction response: {response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
