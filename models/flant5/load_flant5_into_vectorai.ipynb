{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tensorflow as tf\n",
    "from google.cloud import aiplatform\n",
    "import vertexai\n",
    "from models.flant5.FlanT5 import T5FineTuner, tokenize_dataset\n",
    "import os\n",
    "\n",
    "from app.utils.flant5_client import FlanT5Client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Set up paths and configurations\n",
    "LOCAL_MODEL_DIR = os.path.abspath(\"local_model\")\n",
    "SAVED_MODEL_DIR = os.path.abspath(\"saved_model\")\n",
    "GCS_BUCKET = \"ontologykg2\"\n",
    "GCS_MODEL_PATH = f\"gs://{GCS_BUCKET}/model/flant5\"\n",
    "PROJECT_ID = \"deft-return-439619-h9\"\n",
    "REGION = \"us-west1\"\n",
    "\n",
    "# Set up paths using os.path.join for cross-platform compatibility\n",
    "LOCAL_CHECKPOINT_DIR = os.path.abspath(\"local_checkpoint/checkpoint_1000001\")  # Use absolute path\n",
    "LOCAL_EXPORT_DIR = os.path.abspath(\"exported_model\")  # Use absolute path\n",
    "\n",
    "CKPT_PATH = \"models/flant5/epoch=3-step=2072-train_loss=0.35.ckpt\"\n",
    "#CKPT_PATH = \"lightning_logs/version_34/epoch=3-step=2072-train_loss=0.35.ckpt\"\n",
    "#CKPT_PATH = \"lightning_logs/version_30/final.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the local model\n",
    "\n",
    "checkpoint = torch.load(CKPT_PATH)\n",
    "print(checkpoint.keys())\n",
    "\n",
    "llm = T5FineTuner.load_from_checkpoint(CKPT_PATH)\n",
    "\n",
    "llm.model.eval() # set model to evaluation mode\n",
    "llm = llm.to(\"cpu\") # use CPU since I don't have GPU\n",
    "print(\"Done\")\n",
    "\n",
    "model = llm.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Convert PyTorch model to TensorFlow\n",
    "def convert_pt_to_tf(llm):\n",
    "    \"\"\"Convert PyTorch T5 model to TensorFlow\"\"\"\n",
    "    from transformers import TFT5ForConditionalGeneration\n",
    "    \n",
    "    # Create TF model with same config\n",
    "    tf_model = TFT5ForConditionalGeneration.from_pretrained(\n",
    "        llm.hparam.model_name_or_path,\n",
    "        from_pt=True,\n",
    "        config=llm.model.config\n",
    "    )\n",
    "    \n",
    "    # Verify the conversion\n",
    "    print(\"Model converted from PyTorch to TensorFlow\")\n",
    "    print(f\"Model type: {type(tf_model)}\")\n",
    "\n",
    "    return tf_model\n",
    "\n",
    "# Convert PyTorch model to TF\n",
    "tf_model = convert_pt_to_tf(llm)\n",
    "model = tf_model  # Use converted model for serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(input_signature=[{\n",
    "    'input_ids': tf.TensorSpec(shape=(1, 512), dtype=tf.int32, name='input_ids'),\n",
    "    'attention_mask': tf.TensorSpec(shape=(1, 512), dtype=tf.int32, name='attention_mask')\n",
    "}])\n",
    "def serving_fn(inputs):\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_length=128,\n",
    "        num_beams=4,\n",
    "        pad_token_id=model.config.pad_token_id,\n",
    "        eos_token_id=model.config.eos_token_id,\n",
    "        bos_token_id=model.config.bos_token_id,\n",
    "        use_cache=True,\n",
    "        do_sample=False,\n",
    "        num_return_sequences=1,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True\n",
    "    )\n",
    "    return {'sequences': outputs.sequences}\n",
    "\n",
    "# 2.5 Test the serving function\n",
    "def test_serving_fn():\n",
    "    # Create dummy input\n",
    "    dummy_input = {\n",
    "        'input_ids': tf.zeros((1, 512), dtype=tf.int32),\n",
    "        'attention_mask': tf.ones((1, 512), dtype=tf.int32)\n",
    "    }\n",
    "    \n",
    "    # Test the function\n",
    "    result = serving_fn(dummy_input)\n",
    "    print(\"Serving function test successful\")\n",
    "    return result\n",
    "\n",
    "# Test before saving\n",
    "test_serving_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Save as SavedModel format for VectorAI\n",
    "tf.saved_model.save(\n",
    "    model,\n",
    "    SAVED_MODEL_DIR,\n",
    "    signatures={\n",
    "        'serving_default': serving_fn\n",
    "    }\n",
    ")\n",
    "print(f\"SavedModel saved to {SAVED_MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCS_BUCKET = \"ontologykg2\"\n",
    "GCS_MODEL_PATH = f\"gs://{GCS_BUCKET}/model/flant5\"\n",
    "\n",
    "# 4. Upload to GCS and deploy to Vertex AI\n",
    "print(\"Uploading to GCS and deploying to Vertex AI...\")\n",
    "vertexai.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "# Upload and create model\n",
    "model = aiplatform.Model.upload(\n",
    "    display_name=\"flan-t5-base\",\n",
    "    artifact_uri=GCS_MODEL_PATH,\n",
    "    serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/tf2-gpu.2-12:latest\", \n",
    ")\n",
    "\n",
    "# Deploy to endpoint\n",
    "endpoint = model.deploy(\n",
    "    machine_type=\"a2-highgpu-1g\",  # Using GPU machine type for base\n",
    "    accelerator_type=\"NVIDIA_TESLA_A100\",\n",
    "    accelerator_count=1,\n",
    "    min_replica_count=1,\n",
    "    max_replica_count=1,\n",
    ")\n",
    "\n",
    "print(f\"Model deployed to endpoint: {endpoint.resource_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    Uploading to GCS and deploying to Vertex AI...\n",
    "    Creating Model\n",
    "    INFO:google.cloud.aiplatform.models:Creating Model\n",
    "    Create Model backing LRO: projects/371748443295/locations/us-west1/models/5844832289443282944/operations/7813018026546036736\n",
    "    INFO:google.cloud.aiplatform.models:Create Model backing LRO: projects/371748443295/locations/us-west1/models/5844832289443282944/operations/7813018026546036736\n",
    "    Model created. Resource name: projects/371748443295/locations/us-west1/models/5844832289443282944@1\n",
    "    INFO:google.cloud.aiplatform.models:Model created. Resource name: projects/371748443295/locations/us-west1/models/5844832289443282944@1\n",
    "    To use this Model in another session:\n",
    "    INFO:google.cloud.aiplatform.models:To use this Model in another session:\n",
    "    model = aiplatform.Model('projects/371748443295/locations/us-west1/models/5844832289443282944@1')\n",
    "    INFO:google.cloud.aiplatform.models:model = aiplatform.Model('projects/371748443295/locations/us-west1/models/5844832289443282944@1')\n",
    "   \n",
    "    Creating Endpoint\n",
    "    INFO:google.cloud.aiplatform.models:Creating Endpoint\n",
    "    Create Endpoint backing LRO: projects/371748443295/locations/us-west1/endpoints/8518052919822516224/operations/8211586593568325632\n",
    "    INFO:google.cloud.aiplatform.models:Create Endpoint backing LRO: projects/371748443295/locations/us-west1/endpoints/8518052919822516224/operations/8211586593568325632\n",
    "    Endpoint created. Resource name: projects/371748443295/locations/us-west1/endpoints/8518052919822516224\n",
    "    INFO:google.cloud.aiplatform.models:Endpoint created. Resource name: projects/371748443295/locations/us-west1/endpoints/8518052919822516224\n",
    "    To use this Endpoint in another session:\n",
    "    INFO:google.cloud.aiplatform.models:To use this Endpoint in another session:\n",
    "    endpoint = aiplatform.Endpoint('projects/371748443295/locations/us-west1/endpoints/8518052919822516224')\n",
    "    INFO:google.cloud.aiplatform.models:endpoint = aiplatform.Endpoint('projects/371748443295/locations/us-west1/endpoints/8518052919822516224')\n",
    "    Deploying model to Endpoint : projects/371748443295/locations/us-west1/endpoints/8518052919822516224\n",
    "    INFO:google.cloud.aiplatform.models:Deploying model to Endpoint : projects/371748443295/locations/us-west1/endpoints/8518052919822516224\n",
    "    Deploy Endpoint model backing LRO: projects/371748443295/locations/us-west1/endpoints/8518052919822516224/operations/1435920954189414400\n",
    "    INFO:google.cloud.aiplatform.models:Deploy Endpoint model backing LRO: projects/371748443295/locations/us-west1/endpoints/8518052919822516224/operations/1435920954189414400\n",
    "    Endpoint model deployed. Resource name: projects/371748443295/locations/us-west1/endpoints/8518052919822516224\n",
    "    INFO:google.cloud.aiplatform.models:Endpoint model deployed. Resource name: projects/371748443295/locations/us-west1/endpoints/8518052919822516224\n",
    "    Model deployed to endpoint: projects/371748443295/locations/us-west1/endpoints/8518052919822516224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flant5 = FlanT5Client() #uses FLANT5_ENDPOINT set in utils.rag_constants.py by default\n",
    "# Test prediction\n",
    "test_text = \"Translate to French: Hello, how are you?\"\n",
    "response = flant5.generate_content(test_text)\n",
    "print(f\"Test prediction response: {response}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
